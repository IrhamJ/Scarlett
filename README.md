Data Engineering Assessment (DEA) SolutionThis repository contains the complete solution for the Mini-ETL Pipeline case study, demonstrating proficiency in data modeling, incremental processing, streaming simulation (CDC), Python best practices, and Dockerization.1. Project Context and ArchitectureThe solution establishes a modern ELT stack to move data from transactional sources (MySQL/MongoDB) into an analytical data warehouse (ClickHouse).1.1 Core Architecture and Data FlowSource: MySQL (ewallet_source) [3NF]Pipeline: Python (Modular functions, Incremental Logic, Error Handling)Destination: ClickHouse (analytics_dw) [Star Schema]1.2 Setup and Execution (The Full Stack)The entire environment is containerized using Docker Compose. The etl_service container runs the main Python pipeline (etl_pipeline.py).Prerequisites: Docker Desktop and Python (with libraries in requirements.txt) are installed.StepActionCommand1. Build & Run StackBuilds the ETL service and starts all containers (MySQL, MongoDB, ClickHouse, ETL).docker compose up --build -d2. Execute MySQL DDLCreates the Source OLTP schema.`type DDL-MySQL-Source.sql3. Seed MySQL DataPopulates the MySQL source tables.`type seeding-data-MySQL-Source.sql4. Execute ClickHouse DDLCreates the Destination Star Schema and CDC table.`type DDL-Clickhouse.sql5. Run CDC SimulationLoads Change Events into the dedicated CDC table.python cdc_stream.py6. Run ETL Pipeline (Initial/Incremental)Executes the main ELT job inside the Docker container (job runs once).Handled by docker compose up in the etl_service CMD2. Solution for Section 1: Data Modeling & Query Design2.1 ERD Design and JustificationThe source schema follows 3NF for transactional integrity.Relationship Logic: All relationships (USERS $\rightarrow$ TRANSACTIONS and MERCHANTS $\rightarrow$ TRANSACTIONS) are One-to-Many Mandatory.Enforcement: This is enforced using FOREIGN KEY and NOT NULL constraints on the Foreign Key columns in the transactions table.ClickHouse DDL: The destination uses a Star Schema with MergeTree and PARTITION BY transaction_date for analytical speed, using Surrogate Keys instead of Natural Keys.2.2 Data Mapping and Transformation StrategyThe process utilizes an ELT workflow, leveraging Python for complex transformations.Source (MySQL)Destination (ClickHouse)Transformation ActionTransaction Data (3NF)$\rightarrow$ fact_sales & dim_usersDenormalization: Replaced Natural Keys (e.g., user_id) with Surrogate Keys (user_key). Time Split: transaction_time is split into DateTime and Date for ClickHouse partitioning optimization.MongoDB (Conceptual)$\rightarrow$ Dedicated Log TableData is extracted using pymongo, flattened from JSON documents into rows and columns, and then loaded.2.3 Querying (Q1.b)The necessary analytical queries are provided in the repository files (Querying_*.sql).3. Solution for Section 2 & 3: Mini-ETL & Incremental LoadThe etl_pipeline.py script demonstrates an efficient, idempotent, and modular pipeline.3.1 Incremental Load and Checkpointing (Q3)Checkpoint Mechanism: Checkpointing is implemented using a dedicated checkpoint table in MySQL.Watermark Logic: The extract_data() function uses the last_synced_at field from this table as the watermark lower bound (transaction_time > '{last_sync}').Idempotency: The update_watermark() function uses an UPSERT logic (INSERT ON DUPLICATE KEY UPDATE), ensuring that the sync time is atomically updated only after a successful load, preventing duplicates on failure.Validation: The pipeline successfully pulled 3 new rows (TX1006, TX1007, TX1008) during the incremental test run.3.2 Transformation Logic (Q2)The transform_data() function executes business logic on the data:Categorization: A new column, amount_category (Low Value, Medium Value, High Value), is derived from the raw amount.Filtering: Transactions with status = 'FAILED' are filtered out and not loaded into the analytical fact_sales table.4. Solution for Section 4: CDC & Streaming Simulation4.1 Concepts (Q4.a)The decision between Batch and CDC relies on data freshness requirements.What is CDC? I use CDC to detect and track all data modifications (inserts, updates, and deletes) directly from the source databaseâ€™s transaction log. This treats the database as a high-fidelity data stream, offering minimal latency.Batch vs. CDC: I recommend Batch Processing for low-latency tasks (historical reports), while CDC is mandatory for near real-time applications (e.g., fraud detection) where every change must be reflected instantly in the destination system for immediate decision-making.4.2 CDC Simulation (Q4.b)Implementation: The cdc_stream.py script simulates a streaming consumer reading change events from a JSON array (cdc_events.json).Upsert Logic: The script loads data into the cdc_transactions table, which uses the ReplacingMergeTree engine. This engine, combined with the _version timestamp and the OPTIMIZE TABLE command, natively handles Upserts and Deletes by retaining only the latest state of each unique transaction_id.Failure Handling: Failures during the load are handled by a try...except block; in a real-world Kafka environment, failure to commit the offset would trigger automatic re-delivery, preventing data loss.5. Solution for Section 5 & Bonus: Code Quality5.1 Python Best Practices (Code Quality)Python best practices were applied to all execution scripts:Modularity: Logic separated into distinct, reusable functions (extract, transform, load).Logging: All print() statements replaced with the Python logging module (logger.info, logger.error) for structured traceability.Robust Error Handling: Implemented specific try...except blocks to handle database connection failures (MySQL, ClickHouse) and SQL query errors.5.2 Bonus: Unit TestsUnit Test File: test_etl_logic.pyGoal: Successfully ran 2 unit tests using pytest to prove that the core transform_data() logic (categorization and key mocking) works reliably without requiring an active database connection.5.3 Bonus: DockerizationContainerized Service: The ETL code is packaged into the python_etl_pipeline:latest Docker image and executed as the etl_service using Docker Compose. This ensures the pipeline is portable and runs with consistent dependencies.
